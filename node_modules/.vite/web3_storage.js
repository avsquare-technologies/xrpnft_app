import {
  BaseBlockstore,
  CID,
  CarBlockIterator,
  CarReader,
  CarWriter,
  TreewalkCarSplitter,
  UnixFS,
  code,
  code2,
  code3,
  decode,
  decode3 as decode2,
  decode4 as decode3,
  identity,
  importer,
  init_cid,
  init_digest,
  init_identity,
  init_raw,
  init_sha2_browser,
  init_src,
  murmur3128,
  normaliseInput,
  normaliseInput2,
  require_browser_readablestream_to_it,
  require_err_code,
  require_it_last,
  require_it_pipe,
  require_p_retry,
  require_src,
  require_throttledQueue,
  sha256,
  transform
} from "./chunk-JYZO6WQE.js";
import {
  __spreadProps,
  __spreadValues,
  __toESM
} from "./chunk-IGMYUX52.js";

// node_modules/web3.storage/src/lib.js
var import_p_retry = __toESM(require_p_retry(), 1);

// node_modules/ipfs-car/dist/esm/pack/index.js
var import_it_last = __toESM(require_it_last());
var import_it_pipe = __toESM(require_it_pipe());

// node_modules/ipfs-car/dist/esm/pack/utils/normalise-input.js
function isBytes(obj) {
  return ArrayBuffer.isView(obj) || obj instanceof ArrayBuffer;
}
function isBlob(obj) {
  return Boolean(obj.constructor) && (obj.constructor.name === "Blob" || obj.constructor.name === "File") && typeof obj.stream === "function";
}
function isSingle(input) {
  return typeof input === "string" || input instanceof String || isBytes(input) || isBlob(input) || "_readableState" in input;
}
function getNormaliser(input) {
  if (isSingle(input)) {
    return normaliseInput(input);
  } else {
    return normaliseInput2(input);
  }
}

// node_modules/ipfs-car/dist/esm/blockstore/memory.js
init_src();
var MemoryBlockStore = class extends BaseBlockstore {
  constructor() {
    super();
    this.store = /* @__PURE__ */ new Map();
  }
  async *blocks() {
    for (const [cidStr, bytes] of this.store.entries()) {
      yield { cid: CID.parse(cidStr), bytes };
    }
  }
  put(cid, bytes) {
    this.store.set(cid.toString(), bytes);
    return Promise.resolve();
  }
  get(cid) {
    const bytes = this.store.get(cid.toString());
    if (!bytes) {
      throw new Error(`block with cid ${cid.toString()} no found`);
    }
    return Promise.resolve(bytes);
  }
  has(cid) {
    return Promise.resolve(this.store.has(cid.toString()));
  }
  close() {
    this.store.clear();
    return Promise.resolve();
  }
};

// node_modules/ipfs-car/dist/esm/pack/constants.js
init_sha2_browser();
var unixfsImporterOptionsDefault = {
  cidVersion: 1,
  chunker: "fixed",
  maxChunkSize: 262144,
  hasher: sha256,
  rawLeaves: true,
  wrapWithDirectory: true,
  maxChildrenPerNode: 174
};

// node_modules/ipfs-car/dist/esm/pack/index.js
async function pack({ input, blockstore: userBlockstore, hasher, maxChunkSize, maxChildrenPerNode, wrapWithDirectory, rawLeaves }) {
  if (!input || Array.isArray(input) && !input.length) {
    throw new Error("missing input file(s)");
  }
  const blockstore = userBlockstore ? userBlockstore : new MemoryBlockStore();
  const rootEntry = await (0, import_it_last.default)((0, import_it_pipe.default)(getNormaliser(input), (source) => importer(source, blockstore, __spreadProps(__spreadValues({}, unixfsImporterOptionsDefault), {
    hasher: hasher || unixfsImporterOptionsDefault.hasher,
    maxChunkSize: maxChunkSize || unixfsImporterOptionsDefault.maxChunkSize,
    maxChildrenPerNode: maxChildrenPerNode || unixfsImporterOptionsDefault.maxChildrenPerNode,
    wrapWithDirectory: wrapWithDirectory === false ? false : unixfsImporterOptionsDefault.wrapWithDirectory,
    rawLeaves: rawLeaves == null ? unixfsImporterOptionsDefault.rawLeaves : rawLeaves
  }))));
  if (!rootEntry || !rootEntry.cid) {
    throw new Error("given input could not be parsed correctly");
  }
  const root = rootEntry.cid;
  const { writer, out: carOut } = await CarWriter.create([root]);
  const carOutIter = carOut[Symbol.asyncIterator]();
  let writingPromise;
  const writeAll = async () => {
    for await (const block of blockstore.blocks()) {
      await writer.put(block);
    }
    await writer.close();
    if (!userBlockstore) {
      await blockstore.close();
    }
  };
  const out = {
    [Symbol.asyncIterator]() {
      if (writingPromise != null) {
        throw new Error("Multiple iterator not supported");
      }
      writingPromise = writeAll();
      return {
        async next() {
          const result = await carOutIter.next();
          if (result.done) {
            await writingPromise;
          }
          return result;
        }
      };
    }
  };
  return { root, out };
}

// node_modules/@web3-storage/parse-link-header/index.js
var MAX_HEADER_LENGTH = 2e3;
var THROW_ON_MAX_HEADER_LENGTH_EXCEEDED = false;
function hasRel(x) {
  return x && x.rel;
}
function intoRels(acc, x) {
  function splitRel(rel) {
    acc[rel] = Object.assign({}, x, { rel });
  }
  x.rel.split(/\s+/).forEach(splitRel);
  return acc;
}
function createObjects(acc, p) {
  const m = p.match(/\s*(.+)\s*=\s*"?([^"]+)"?/);
  if (m)
    acc[m[1]] = m[2];
  return acc;
}
function parseLink(link) {
  try {
    const m = link.match(/<?([^>]*)>(.*)/);
    const linkUrl = m[1];
    const parts = m[2].split(";");
    const qry = {};
    const url = new URL(linkUrl, "https://example.com");
    for (const [key, value] of url.searchParams) {
      qry[key] = value;
    }
    parts.shift();
    let info = parts.reduce(createObjects, {});
    info = Object.assign({}, qry, info);
    info.url = linkUrl;
    return info;
  } catch {
    return null;
  }
}
function checkHeader(linkHeader, options) {
  if (!linkHeader)
    return false;
  options = options || {};
  const maxHeaderLength = options.maxHeaderLength || MAX_HEADER_LENGTH;
  const throwOnMaxHeaderLengthExceeded = options.throwOnMaxHeaderLengthExceeded || THROW_ON_MAX_HEADER_LENGTH_EXCEEDED;
  if (linkHeader.length > maxHeaderLength) {
    if (throwOnMaxHeaderLengthExceeded) {
      throw new Error("Input string too long, it should be under " + maxHeaderLength + " characters.");
    } else {
      return false;
    }
  }
  return true;
}
function parseLinkHeader(linkHeader, options) {
  if (!checkHeader(linkHeader, options))
    return null;
  return linkHeader.split(/,\s*</).map(parseLink).filter(hasRel).reduce(intoRels, {});
}

// node_modules/ipfs-car/dist/esm/unpack/index.js
var import_browser_readablestream_to_it = __toESM(require_browser_readablestream_to_it());

// node_modules/ipfs-unixfs-exporter/esm/src/index.js
var import_err_code8 = __toESM(require_err_code());
init_cid();

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/index.js
var import_err_code7 = __toESM(require_err_code(), 1);
init_raw();
init_identity();

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/unixfs-v1/index.js
var import_err_code3 = __toESM(require_err_code(), 1);

// node_modules/ipfs-unixfs-exporter/esm/src/utils/find-cid-in-shard.js
var import_hamt_sharding = __toESM(require_src(), 1);
var hashFn = async function(buf) {
  return (await murmur3128.encode(buf)).slice(0, 8).reverse();
};
var addLinksToHamtBucket = (links, bucket, rootBucket) => {
  return Promise.all(links.map((link) => {
    if (link.Name == null) {
      throw new Error("Unexpected Link without a Name");
    }
    if (link.Name.length === 2) {
      const pos = parseInt(link.Name, 16);
      return bucket._putObjectAt(pos, new import_hamt_sharding.Bucket({
        hash: rootBucket._options.hash,
        bits: rootBucket._options.bits
      }, bucket, pos));
    }
    return rootBucket.put(link.Name.substring(2), true);
  }));
};
var toPrefix = (position) => {
  return position.toString(16).toUpperCase().padStart(2, "0").substring(0, 2);
};
var toBucketPath = (position) => {
  let bucket = position.bucket;
  const path = [];
  while (bucket._parent) {
    path.push(bucket);
    bucket = bucket._parent;
  }
  path.push(bucket);
  return path.reverse();
};
var findShardCid = async (node, name, blockstore, context, options) => {
  if (!context) {
    const rootBucket = (0, import_hamt_sharding.createHAMT)({ hashFn });
    context = {
      rootBucket,
      hamtDepth: 1,
      lastBucket: rootBucket
    };
  }
  await addLinksToHamtBucket(node.Links, context.lastBucket, context.rootBucket);
  const position = await context.rootBucket._findNewBucketAndPos(name);
  let prefix = toPrefix(position.pos);
  const bucketPath = toBucketPath(position);
  if (bucketPath.length > context.hamtDepth) {
    context.lastBucket = bucketPath[context.hamtDepth];
    prefix = toPrefix(context.lastBucket._posAtParent);
  }
  const link = node.Links.find((link2) => {
    if (link2.Name == null) {
      return false;
    }
    const entryPrefix = link2.Name.substring(0, 2);
    const entryName = link2.Name.substring(2);
    if (entryPrefix !== prefix) {
      return false;
    }
    if (entryName && entryName !== name) {
      return false;
    }
    return true;
  });
  if (!link) {
    return null;
  }
  if (link.Name != null && link.Name.substring(2) === name) {
    return link.Hash;
  }
  context.hamtDepth++;
  const block = await blockstore.get(link.Hash, options);
  node = decode3(block);
  return findShardCid(node, name, blockstore, context, options);
};
var find_cid_in_shard_default = findShardCid;

// node_modules/ipfs-unixfs-exporter/esm/src/utils/extract-data-from-block.js
function extractDataFromBlock(block, blockStart, requestedStart, requestedEnd) {
  const blockLength = block.length;
  const blockEnd = blockStart + blockLength;
  if (requestedStart >= blockEnd || requestedEnd < blockStart) {
    return new Uint8Array(0);
  }
  if (requestedEnd >= blockStart && requestedEnd < blockEnd) {
    block = block.slice(0, requestedEnd - blockStart);
  }
  if (requestedStart >= blockStart && requestedStart < blockEnd) {
    block = block.slice(requestedStart - blockStart);
  }
  return block;
}
var extract_data_from_block_default = extractDataFromBlock;

// node_modules/ipfs-unixfs-exporter/esm/src/utils/validate-offset-and-length.js
var import_err_code = __toESM(require_err_code(), 1);
var validateOffsetAndLength = (size, offset, length) => {
  if (!offset) {
    offset = 0;
  }
  if (offset < 0) {
    throw (0, import_err_code.default)(new Error("Offset must be greater than or equal to 0"), "ERR_INVALID_PARAMS");
  }
  if (offset > size) {
    throw (0, import_err_code.default)(new Error("Offset must be less than the file size"), "ERR_INVALID_PARAMS");
  }
  if (!length && length !== 0) {
    length = size - offset;
  }
  if (length < 0) {
    throw (0, import_err_code.default)(new Error("Length must be greater than or equal to 0"), "ERR_INVALID_PARAMS");
  }
  if (offset + length > size) {
    length = size - offset;
  }
  return {
    offset,
    length
  };
};
var validate_offset_and_length_default = validateOffsetAndLength;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/unixfs-v1/content/file.js
var import_err_code2 = __toESM(require_err_code(), 1);
init_raw();
async function* emitBytes(blockstore, node, start, end, streamPosition = 0, options) {
  if (node instanceof Uint8Array) {
    const buf = extract_data_from_block_default(node, streamPosition, start, end);
    if (buf.length) {
      yield buf;
    }
    streamPosition += buf.length;
    return streamPosition;
  }
  if (node.Data == null) {
    throw (0, import_err_code2.default)(new Error("no data in PBNode"), "ERR_NOT_UNIXFS");
  }
  let file;
  try {
    file = UnixFS.unmarshal(node.Data);
  } catch (err) {
    throw (0, import_err_code2.default)(err, "ERR_NOT_UNIXFS");
  }
  if (file.data && file.data.length) {
    const buf = extract_data_from_block_default(file.data, streamPosition, start, end);
    if (buf.length) {
      yield buf;
    }
    streamPosition += file.data.length;
  }
  let childStart = streamPosition;
  for (let i = 0; i < node.Links.length; i++) {
    const childLink = node.Links[i];
    const childEnd = streamPosition + file.blockSizes[i];
    if (start >= childStart && start < childEnd || end > childStart && end <= childEnd || start < childStart && end > childEnd) {
      const block = await blockstore.get(childLink.Hash, { signal: options.signal });
      let child;
      switch (childLink.Hash.code) {
        case code2:
          child = await decode3(block);
          break;
        case code3:
          child = block;
          break;
        case code:
          child = await decode2(block);
          break;
        default:
          throw Error(`Unsupported codec: ${childLink.Hash.code}`);
      }
      for await (const buf of emitBytes(blockstore, child, start, end, streamPosition, options)) {
        streamPosition += buf.length;
        yield buf;
      }
    }
    streamPosition = childEnd;
    childStart = childEnd + 1;
  }
}
var fileContent = (cid, node, unixfs, path, resolve5, depth, blockstore) => {
  function yieldFileContent(options = {}) {
    const fileSize = unixfs.fileSize();
    if (fileSize === void 0) {
      throw new Error("File was a directory");
    }
    const { offset, length } = validate_offset_and_length_default(fileSize, options.offset, options.length);
    const start = offset;
    const end = offset + length;
    return emitBytes(blockstore, node, start, end, 0, options);
  }
  return yieldFileContent;
};
var file_default = fileContent;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/unixfs-v1/content/directory.js
var directoryContent = (cid, node, unixfs, path, resolve5, depth, blockstore) => {
  async function* yieldDirectoryContent(options = {}) {
    const offset = options.offset || 0;
    const length = options.length || node.Links.length;
    const links = node.Links.slice(offset, length);
    for (const link of links) {
      const result = await resolve5(link.Hash, link.Name || "", `${path}/${link.Name || ""}`, [], depth + 1, blockstore, options);
      if (result.entry) {
        yield result.entry;
      }
    }
  }
  return yieldDirectoryContent;
};
var directory_default = directoryContent;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/unixfs-v1/content/hamt-sharded-directory.js
var hamtShardedDirectoryContent = (cid, node, unixfs, path, resolve5, depth, blockstore) => {
  function yieldHamtDirectoryContent(options = {}) {
    return listDirectory(node, path, resolve5, depth, blockstore, options);
  }
  return yieldHamtDirectoryContent;
};
async function* listDirectory(node, path, resolve5, depth, blockstore, options) {
  const links = node.Links;
  for (const link of links) {
    const name = link.Name != null ? link.Name.substring(2) : null;
    if (name) {
      const result = await resolve5(link.Hash, name, `${path}/${name}`, [], depth + 1, blockstore, options);
      yield result.entry;
    } else {
      const block = await blockstore.get(link.Hash);
      node = decode3(block);
      for await (const file of listDirectory(node, path, resolve5, depth, blockstore, options)) {
        yield file;
      }
    }
  }
}
var hamt_sharded_directory_default = hamtShardedDirectoryContent;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/unixfs-v1/index.js
var findLinkCid = (node, name) => {
  const link = node.Links.find((link2) => link2.Name === name);
  return link && link.Hash;
};
var contentExporters = {
  raw: file_default,
  file: file_default,
  directory: directory_default,
  "hamt-sharded-directory": hamt_sharded_directory_default,
  metadata: (cid, node, unixfs, path, resolve5, depth, blockstore) => {
    return () => [];
  },
  symlink: (cid, node, unixfs, path, resolve5, depth, blockstore) => {
    return () => [];
  }
};
var unixFsResolver = async (cid, name, path, toResolve, resolve5, depth, blockstore, options) => {
  const block = await blockstore.get(cid, options);
  const node = decode3(block);
  let unixfs;
  let next;
  if (!name) {
    name = cid.toString();
  }
  if (node.Data == null) {
    throw (0, import_err_code3.default)(new Error("no data in PBNode"), "ERR_NOT_UNIXFS");
  }
  try {
    unixfs = UnixFS.unmarshal(node.Data);
  } catch (err) {
    throw (0, import_err_code3.default)(err, "ERR_NOT_UNIXFS");
  }
  if (!path) {
    path = name;
  }
  if (toResolve.length) {
    let linkCid;
    if (unixfs && unixfs.type === "hamt-sharded-directory") {
      linkCid = await find_cid_in_shard_default(node, toResolve[0], blockstore);
    } else {
      linkCid = findLinkCid(node, toResolve[0]);
    }
    if (!linkCid) {
      throw (0, import_err_code3.default)(new Error("file does not exist"), "ERR_NOT_FOUND");
    }
    const nextName = toResolve.shift();
    const nextPath = `${path}/${nextName}`;
    next = {
      cid: linkCid,
      toResolve,
      name: nextName || "",
      path: nextPath
    };
  }
  return {
    entry: {
      type: unixfs.isDirectory() ? "directory" : "file",
      name,
      path,
      cid,
      content: contentExporters[unixfs.type](cid, node, unixfs, path, resolve5, depth, blockstore),
      unixfs,
      depth,
      node,
      size: unixfs.fileSize()
    },
    next
  };
};
var unixfs_v1_default = unixFsResolver;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/raw.js
var import_err_code4 = __toESM(require_err_code(), 1);
var rawContent = (node) => {
  async function* contentGenerator(options = {}) {
    const { offset, length } = validate_offset_and_length_default(node.length, options.offset, options.length);
    yield extract_data_from_block_default(node, 0, offset, offset + length);
  }
  return contentGenerator;
};
var resolve = async (cid, name, path, toResolve, resolve5, depth, blockstore, options) => {
  if (toResolve.length) {
    throw (0, import_err_code4.default)(new Error(`No link named ${path} found in raw node ${cid}`), "ERR_NOT_FOUND");
  }
  const block = await blockstore.get(cid, options);
  return {
    entry: {
      type: "raw",
      name,
      path,
      cid,
      content: rawContent(block),
      depth,
      size: block.length,
      node: block
    }
  };
};
var raw_default = resolve;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/dag-cbor.js
init_cid();
var import_err_code5 = __toESM(require_err_code(), 1);
var resolve2 = async (cid, name, path, toResolve, resolve5, depth, blockstore, options) => {
  const block = await blockstore.get(cid);
  const object = decode2(block);
  let subObject = object;
  let subPath = path;
  while (toResolve.length) {
    const prop = toResolve[0];
    if (prop in subObject) {
      toResolve.shift();
      subPath = `${subPath}/${prop}`;
      const subObjectCid = CID.asCID(subObject[prop]);
      if (subObjectCid) {
        return {
          entry: {
            type: "object",
            name,
            path,
            cid,
            node: block,
            depth,
            size: block.length,
            content: async function* () {
              yield object;
            }
          },
          next: {
            cid: subObjectCid,
            name: prop,
            path: subPath,
            toResolve
          }
        };
      }
      subObject = subObject[prop];
    } else {
      throw (0, import_err_code5.default)(new Error(`No property named ${prop} found in cbor node ${cid}`), "ERR_NO_PROP");
    }
  }
  return {
    entry: {
      type: "object",
      name,
      path,
      cid,
      node: block,
      depth,
      size: block.length,
      content: async function* () {
        yield object;
      }
    }
  };
};
var dag_cbor_default = resolve2;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/identity.js
var import_err_code6 = __toESM(require_err_code(), 1);
init_digest();
var rawContent2 = (node) => {
  async function* contentGenerator(options = {}) {
    const { offset, length } = validate_offset_and_length_default(node.length, options.offset, options.length);
    yield extract_data_from_block_default(node, 0, offset, offset + length);
  }
  return contentGenerator;
};
var resolve3 = async (cid, name, path, toResolve, resolve5, depth, blockstore, options) => {
  if (toResolve.length) {
    throw (0, import_err_code6.default)(new Error(`No link named ${path} found in raw node ${cid}`), "ERR_NOT_FOUND");
  }
  const buf = await decode(cid.multihash.bytes);
  return {
    entry: {
      type: "identity",
      name,
      path,
      cid,
      content: rawContent2(buf.digest),
      depth,
      size: buf.digest.length,
      node: buf.digest
    }
  };
};
var identity_default = resolve3;

// node_modules/ipfs-unixfs-exporter/esm/src/resolvers/index.js
var resolvers = {
  [code2]: unixfs_v1_default,
  [code3]: raw_default,
  [code]: dag_cbor_default,
  [identity.code]: identity_default
};
function resolve4(cid, name, path, toResolve, depth, blockstore, options) {
  const resolver = resolvers[cid.code];
  if (!resolver) {
    throw (0, import_err_code7.default)(new Error(`No resolver for code ${cid.code}`), "ERR_NO_RESOLVER");
  }
  return resolver(cid, name, path, toResolve, resolve4, depth, blockstore, options);
}
var resolvers_default = resolve4;

// node_modules/ipfs-unixfs-exporter/esm/src/index.js
var import_it_last2 = __toESM(require_it_last());
var toPathComponents = (path = "") => {
  return (path.trim().match(/([^\\^/]|\\\/)+/g) || []).filter(Boolean);
};
var cidAndRest = (path) => {
  if (path instanceof Uint8Array) {
    return {
      cid: CID.decode(path),
      toResolve: []
    };
  }
  const cid = CID.asCID(path);
  if (cid) {
    return {
      cid,
      toResolve: []
    };
  }
  if (typeof path === "string") {
    if (path.indexOf("/ipfs/") === 0) {
      path = path.substring(6);
    }
    const output = toPathComponents(path);
    return {
      cid: CID.parse(output[0]),
      toResolve: output.slice(1)
    };
  }
  throw (0, import_err_code8.default)(new Error(`Unknown path type ${path}`), "ERR_BAD_PATH");
};
async function* walkPath(path, blockstore, options = {}) {
  let { cid, toResolve } = cidAndRest(path);
  let name = cid.toString();
  let entryPath = name;
  const startingDepth = toResolve.length;
  while (true) {
    const result = await resolvers_default(cid, name, entryPath, toResolve, startingDepth, blockstore, options);
    if (!result.entry && !result.next) {
      throw (0, import_err_code8.default)(new Error(`Could not resolve ${path}`), "ERR_NOT_FOUND");
    }
    if (result.entry) {
      yield result.entry;
    }
    if (!result.next) {
      return;
    }
    toResolve = result.next.toResolve;
    cid = result.next.cid;
    name = result.next.name;
    entryPath = result.next.path;
  }
}
async function exporter(path, blockstore, options = {}) {
  const result = await (0, import_it_last2.default)(walkPath(path, blockstore, options));
  if (!result) {
    throw (0, import_err_code8.default)(new Error(`Could not resolve ${path}`), "ERR_NOT_FOUND");
  }
  return result;
}
async function* recursive(path, blockstore, options = {}) {
  const node = await exporter(path, blockstore, options);
  if (!node) {
    return;
  }
  yield node;
  if (node.type === "directory") {
    for await (const child of recurse(node, options)) {
      yield child;
    }
  }
  async function* recurse(node2, options2) {
    for await (const file of node2.content(options2)) {
      yield file;
      if (file instanceof Uint8Array) {
        continue;
      }
      if (file.type === "directory") {
        yield* recurse(file, options2);
      }
    }
  }
}

// node_modules/uint8arrays/esm/src/equals.js
function equals(a, b) {
  if (a === b) {
    return true;
  }
  if (a.byteLength !== b.byteLength) {
    return false;
  }
  for (let i = 0; i < a.byteLength; i++) {
    if (a[i] !== b[i]) {
      return false;
    }
  }
  return true;
}

// node_modules/ipfs-car/dist/esm/unpack/utils/verifying-get-only-blockstore.js
init_sha2_browser();
var VerifyingGetOnlyBlockStore = class extends BaseBlockstore {
  constructor(blockstore) {
    super();
    this.store = blockstore;
  }
  async get(cid) {
    const res = await this.store.get(cid);
    if (!res) {
      throw new Error(`Incomplete CAR. Block missing for CID ${cid}`);
    }
    if (!isValid({ cid, bytes: res })) {
      throw new Error(`Invalid CAR. Hash of block data does not match CID ${cid}`);
    }
    return res;
  }
  static fromBlockstore(b) {
    return new VerifyingGetOnlyBlockStore(b);
  }
  static fromCarReader(cr) {
    return new VerifyingGetOnlyBlockStore({
      get: async (cid) => {
        const block = await cr.get(cid);
        return block === null || block === void 0 ? void 0 : block.bytes;
      }
    });
  }
};
async function isValid({ cid, bytes }) {
  const hash = await sha256.digest(bytes);
  return equals(hash.digest, cid.multihash.digest);
}

// node_modules/ipfs-car/dist/esm/unpack/index.js
async function* unpackStream(readable, { roots, blockstore: userBlockstore } = {}) {
  const carIterator = await CarBlockIterator.fromIterable(asAsyncIterable(readable));
  const blockstore = userBlockstore || new MemoryBlockStore();
  for await (const block of carIterator) {
    await blockstore.put(block.cid, block.bytes);
  }
  const verifyingBlockStore = VerifyingGetOnlyBlockStore.fromBlockstore(blockstore);
  if (!roots || roots.length === 0) {
    roots = await carIterator.getRoots();
  }
  for (const root of roots) {
    yield* recursive(root, verifyingBlockStore);
  }
}
function asAsyncIterable(readable) {
  return Symbol.asyncIterator in readable ? readable : (0, import_browser_readablestream_to_it.default)(readable);
}

// node_modules/files-from-path/esm/src/index.browser.js
async function getFilesFromPath() {
  throw new Error("Unsupported in this environment");
}
async function* filesFromPath() {
  throw new Error("Unsupported in this environment");
}

// node_modules/web3.storage/src/lib.js
var import_throttled_queue = __toESM(require_throttledQueue(), 1);

// node_modules/web3.storage/src/platform.web.js
var fetch = globalThis.fetch;
var Request = globalThis.Request;
var Response = globalThis.Response;
var Blob = globalThis.Blob;
var File = globalThis.File;
var Blockstore = MemoryBlockStore;

// node_modules/web3.storage/src/lib.js
var MAX_PUT_RETRIES = 5;
var MAX_CONCURRENT_UPLOADS = 3;
var DEFAULT_CHUNK_SIZE = 1024 * 1024 * 10;
var MAX_BLOCK_SIZE = 1048576;
var MAX_CHUNK_SIZE = 104857600;
var RATE_LIMIT_REQUESTS = 30;
var RATE_LIMIT_PERIOD = 10 * 1e3;
function createRateLimiter() {
  const throttle = (0, import_throttled_queue.default)(RATE_LIMIT_REQUESTS, RATE_LIMIT_PERIOD);
  return () => throttle(() => {
  });
}
var globalRateLimiter = createRateLimiter();
var Web3Storage = class {
  constructor({
    token,
    endpoint = new URL("https://api.web3.storage"),
    rateLimiter,
    fetch: fetch2 = fetch
  }) {
    this.token = token;
    this.endpoint = endpoint;
    this.rateLimiter = rateLimiter || createRateLimiter();
    this.fetch = fetch2;
  }
  static headers(token) {
    if (!token)
      throw new Error("missing token");
    return {
      Authorization: `Bearer ${token}`,
      "X-Client": "web3.storage/js"
    };
  }
  static async put({ endpoint, token, rateLimiter = globalRateLimiter, fetch: fetch2 = fetch }, files, {
    onRootCidReady,
    onStoredChunk,
    maxRetries = MAX_PUT_RETRIES,
    maxChunkSize = DEFAULT_CHUNK_SIZE,
    wrapWithDirectory = true,
    name,
    signal
  } = {}) {
    if (maxChunkSize >= MAX_CHUNK_SIZE || maxChunkSize < MAX_BLOCK_SIZE) {
      throw new Error("maximum chunk size must be less than 100MiB and greater than or equal to 1MB");
    }
    const blockstore = new Blockstore();
    try {
      const { out, root } = await pack({
        input: Array.from(files).map(toImportCandidate),
        blockstore,
        wrapWithDirectory,
        maxChunkSize: MAX_BLOCK_SIZE,
        maxChildrenPerNode: 1024
      });
      onRootCidReady && onRootCidReady(root.toString());
      const car = await CarReader.fromIterable(out);
      return await Web3Storage.putCar({ endpoint, token, rateLimiter, fetch: fetch2 }, car, { onStoredChunk, maxRetries, maxChunkSize, name, signal });
    } finally {
      await blockstore.close();
    }
  }
  static async putCar({ endpoint, token, rateLimiter = globalRateLimiter, fetch: fetch2 = fetch }, car, {
    name,
    onStoredChunk,
    maxRetries = MAX_PUT_RETRIES,
    maxChunkSize = DEFAULT_CHUNK_SIZE,
    decoders,
    signal
  } = {}) {
    if (maxChunkSize >= MAX_CHUNK_SIZE || maxChunkSize < MAX_BLOCK_SIZE) {
      throw new Error("maximum chunk size must be less than 100MiB and greater than or equal to 1MB");
    }
    const targetSize = maxChunkSize;
    const url = new URL("car", endpoint);
    let headers = Web3Storage.headers(token);
    if (name) {
      headers = __spreadProps(__spreadValues({}, headers), { "X-Name": encodeURIComponent(name) });
    }
    const roots = await car.getRoots();
    if (roots[0] == null) {
      throw new Error("missing root CID");
    }
    if (roots.length > 1) {
      throw new Error("too many roots");
    }
    const carRoot = roots[0].toString();
    const splitter = new TreewalkCarSplitter(car, targetSize, { decoders });
    const onCarChunk = async (car2) => {
      const carParts = [];
      for await (const part of car2) {
        carParts.push(part);
      }
      const carFile = new Blob(carParts, { type: "application/vnd.ipld.car" });
      const res = await (0, import_p_retry.default)(async () => {
        await rateLimiter();
        let response;
        try {
          response = await fetch2(url.toString(), {
            method: "POST",
            headers,
            body: carFile,
            signal
          });
        } catch (err) {
          throw signal && signal.aborted ? new import_p_retry.AbortError(err) : err;
        }
        if (response.status === 429) {
          throw new Error("rate limited");
        }
        const res2 = await response.json();
        if (!response.ok) {
          throw new Error(res2.message);
        }
        if (res2.cid !== carRoot) {
          throw new Error(`root CID mismatch, expected: ${carRoot}, received: ${res2.cid}`);
        }
        return res2.cid;
      }, { retries: maxRetries });
      onStoredChunk && onStoredChunk(carFile.size);
      return res;
    };
    const upload = transform(MAX_CONCURRENT_UPLOADS, onCarChunk);
    for await (const _ of upload(splitter.cars())) {
    }
    return carRoot;
  }
  static async get({ endpoint, token, rateLimiter = globalRateLimiter, fetch: fetch2 = fetch }, cid, options = {}) {
    const url = new URL(`car/${cid}`, endpoint);
    await rateLimiter();
    const res = await fetch2(url.toString(), {
      method: "GET",
      headers: Web3Storage.headers(token),
      signal: options.signal
    });
    if (res.status === 429) {
      throw new Error("rate limited");
    }
    return toWeb3Response(res);
  }
  static async delete({ endpoint, token, rateLimiter = globalRateLimiter }, cid, options = {}) {
    console.log("Not deleting", cid, endpoint, token, rateLimiter, options);
    throw Error(".delete not implemented yet");
  }
  static async status({ endpoint, token, rateLimiter = globalRateLimiter, fetch: fetch2 = fetch }, cid, options = {}) {
    const url = new URL(`status/${cid}`, endpoint);
    await rateLimiter();
    const res = await fetch2(url.toString(), {
      method: "GET",
      headers: Web3Storage.headers(token),
      signal: options.signal
    });
    if (res.status === 429) {
      throw new Error("rate limited");
    }
    if (res.status === 404) {
      return void 0;
    }
    if (!res.ok) {
      throw new Error(res.statusText);
    }
    return res.json();
  }
  static async *list(service, { before = new Date().toISOString(), maxResults = Infinity, signal } = {}) {
    async function listPage({ endpoint, token, rateLimiter = globalRateLimiter, fetch: fetch2 = fetch }, { before: before2, size: size2 }) {
      const search = new URLSearchParams({ before: before2, size: size2.toString() });
      const url = new URL(`user/uploads?${search}`, endpoint);
      await rateLimiter();
      return fetch2(url.toString(), {
        method: "GET",
        headers: __spreadProps(__spreadValues({}, Web3Storage.headers(token)), {
          "Access-Control-Request-Headers": "Link"
        }),
        signal
      });
    }
    let count = 0;
    const size = maxResults > 100 ? 100 : maxResults;
    for await (const res of paginator(listPage, service, { before, size })) {
      if (!res.ok) {
        if (res.status === 429) {
          throw new Error("rate limited");
        }
        const errorMessage = await res.json();
        throw new Error(`${res.status} ${res.statusText} ${errorMessage ? "- " + errorMessage.message : ""}`);
      }
      const page = await res.json();
      for (const upload of page) {
        if (++count > maxResults) {
          return;
        }
        yield upload;
      }
    }
  }
  put(files, options) {
    return Web3Storage.put(this, files, options);
  }
  putCar(car, options) {
    return Web3Storage.putCar(this, car, options);
  }
  get(cid, options) {
    return Web3Storage.get(this, cid, options);
  }
  delete(cid, options) {
    return Web3Storage.delete(this, cid, options);
  }
  status(cid, options) {
    return Web3Storage.status(this, cid, options);
  }
  list(opts) {
    return Web3Storage.list(this, opts);
  }
};
async function toWeb3File({ content, path, cid }) {
  const chunks = [];
  for await (const chunk of content()) {
    chunks.push(chunk);
  }
  const file = new File(chunks, toFilenameWithPath(path));
  return Object.assign(file, { cid: cid.toString() });
}
function toFilenameWithPath(unixFsPath) {
  const slashIndex = unixFsPath.indexOf("/");
  return slashIndex === -1 ? unixFsPath : unixFsPath.substring(slashIndex + 1);
}
function toWeb3Response(res) {
  const response = Object.assign(res, {
    unixFsIterator: async function* () {
      if (!res.ok) {
        throw new Error(`Response was not ok: ${res.status} ${res.statusText} - Check for { "ok": false } on the Response object before calling .unixFsIterator`);
      }
      if (!res.body) {
        throw new Error("No body on response");
      }
      const blockstore = new Blockstore();
      try {
        for await (const entry of unpackStream(res.body, { blockstore })) {
          yield entry;
        }
      } finally {
        await blockstore.close();
      }
    },
    files: async () => {
      if (!res.ok) {
        throw new Error(`Response was not ok: ${res.status} ${res.statusText} - Check for { "ok": false } on the Response object before calling .files`);
      }
      const files = [];
      for await (const entry of response.unixFsIterator()) {
        if (entry.type === "directory") {
          continue;
        }
        const file = await toWeb3File(entry);
        files.push(file);
      }
      return files;
    }
  });
  return response;
}
function toImportCandidate(file) {
  let stream;
  return {
    path: file.name,
    get content() {
      stream = stream || file.stream();
      return stream;
    }
  };
}
async function* paginator(fn, service, opts) {
  let res = await fn(service, opts);
  yield res;
  let link = parseLinkHeader(res.headers.get("Link") || "");
  while (link && link.next) {
    res = await fn(service, link.next);
    yield res;
    link = parseLinkHeader(res.headers.get("Link") || "");
  }
}
export {
  Blob,
  File,
  Web3Storage,
  createRateLimiter,
  filesFromPath,
  getFilesFromPath
};
//# sourceMappingURL=web3_storage.js.map
